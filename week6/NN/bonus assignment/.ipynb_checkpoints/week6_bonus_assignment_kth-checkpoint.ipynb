{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 안녕하세요 투빅스 보충 과제입니다 :)\n",
    "\n",
    "안녕하세요 투빅스 12기 김태한입니다 :)\n",
    "\n",
    "이번 과제는 코로나 바이러스로 예상치 못한 휴식시간이 생겨 여러분의 딥러닝 감을 유지하고자 드리게 되었습니다.  \n",
    "\n",
    "투빅이분들이라면 분명 쉽게 해낼거라 믿습니다!!\n",
    "\n",
    "\n",
    "모르시는 거 있으시면 저 그리고 12기 멘토분들을 많이 많이 괴롭혀주세요!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러분들은 저번 과제로 뉴럴넷 구현을 이미 한번 하셨습니다!  \n",
    "\n",
    "사실 이번 과제의 최종 목적도 뉴럴넷 구현인데요 이미 한번 하셨고 실력들이 워낙 출중하셔서 금방금방 하실수 있으실거에요.  \n",
    "\n",
    "구현에 바로 들어가기에 앞서 전체 네트워크 구조와 각 구성요소의 행렬 차원 및 오차역전파(back propagation) 복습이 1번 과제입니다.  \n",
    "\n",
    "**?** 에 들어갈 수식을 채워주시면 됩니다!!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Shape 정리\n",
    "\n",
    "n : sample_data 수  \n",
    "d : input_dimension  \n",
    "h : hidden_layer_dimension  \n",
    "c : output_dimension  \n",
    "\n",
    "X : input_data  \n",
    "W1 : layer1_weight  \n",
    "b1 : layer1_bias  \n",
    "H : X*W1+b1\n",
    "A : activation function 거친 value\n",
    "W2 : layer2_weight  \n",
    "b2 : layer2_bias  \n",
    "S : A*W2+b2  \n",
    "P : softmax 거친 value  \n",
    "\n",
    "**X==(n,d)  \n",
    "W1==(?,?) 채워주세요  \n",
    "b1==(h,)  \n",
    "H==(?,?) 채워주세요  \n",
    "A==(n,h)  \n",
    "W2==(h,c)  \n",
    "b2==(?,) 채워주세요  \n",
    "S==(?,?) 채워주세요  \n",
    "P==(n,c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix 미분 정리\n",
    "$H = XW+b　　　(n,h) = (n,d)x(d,h)+(h,)$  \n",
    "$L = f(H)$  \n",
    "$\\frac{\\partial L}{\\partial W} = \\frac{\\partial H}{\\partial W} \\times \\frac{\\partial L}{\\partial H} = ?\\frac{\\partial L}{\\partial H}$ 　채워주세요  \n",
    "$\\frac{\\partial L}{\\partial X} = \\frac{\\partial H}{\\partial W} \\times \\frac{\\partial L}{\\partial H} = \\frac{\\partial L}{\\partial H}?$ 　채워주세요  \n",
    "$\\frac{\\partial L}{\\partial b} = 1*\\frac{\\partial L}{\\partial H}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Layers Chain Rule 정리\n",
    "**Forward** \n",
    "\n",
    "$H = XW_{1} + b$  \n",
    "$A = ReLU(H)$  \n",
    "$S = AW_{2} + b_{2}$  \n",
    "$P = Softmax(S)$  \n",
    "$L = -LogLikelihood(P)$\n",
    "\n",
    "\n",
    "**Backward**\n",
    "\n",
    "$\\frac{\\partial L}{\\partial S} = P-T$　:　T는 Label  \n",
    "$\\frac{\\partial L}{\\partial W_{2}} = \\frac{\\partial S}{\\partial W_{2}}\\frac{\\partial L}{\\partial S} = ?$ 　채워주세요  \n",
    "$\\frac{\\partial L}{\\partial b_{2}} = 1*\\frac{\\partial L}{\\partial S} = P-T$  \n",
    "$\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial S}\\frac{\\partial S}{\\partial A} = ?$　채워주세요  \n",
    "$\\frac{\\partial L}{\\partial H} = \\frac{\\partial A}{\\partial H}\\frac{\\partial L}{\\partial A}$  \n",
    "$\\frac{\\partial L}{\\partial W_{1}} = \\frac{\\partial H}{\\partial W_{1}}\\frac{\\partial L}{\\partial H} = X^{T}\\frac{\\partial L}{\\partial H}$  \n",
    "$\\frac{\\partial L}{\\partial b_{1}} = \\frac{\\partial L}{\\partial H}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같이 드린 파일중 model.py라는 파일이 있을거에요!!!  \n",
    "그 친구의 빈칸을 채워주시면 되겠습니다~!!  \n",
    "model.py의 함수는 assignment3의 모델 만들기에서 사용되니 참고하시면서 채워주시면 도움이 될거에요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 이제 저희가 구현한 모델을 가지고 한번 cifar-10 dataset을 학습해볼게요!!  \n",
    "근데 시작하기에 앞서 pip install keras 를 해주세요!!  \n",
    "\n",
    "3번과제의 목적은 하이퍼파라미터를 튜닝하던 다른방법을 사용하던 해서 마지막에 그림그리기에서 높은 validation accuracy가 나오도록 하는 과제입니다!!  \n",
    "\n",
    "모델을 2층이아니라 본인만의 3층으로 발전시켜도 좋구요 다른 여러가지 방법들이 있겠죠!?!?!?  \n",
    "\n",
    "가장 높은 validation accuracy를 뽑으신 분께 상품을 드리겠습니다아~!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 load\n",
    "\n",
    "keras 프레임워크를 이용하여 데이터를 로드해 옵니다.  \n",
    "32*32*3차원의 데이터를 3072차원으로 바꾸는 것 까지 해드릴게요.\n",
    "필요하면 sklearn.preprocessing의 scaler를 사용해보셔도 좋습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from Model import TwoLayerNet\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(x_train, x_test, y_train, y_test):\n",
    "    #change dtype\n",
    "    x_train = np.array(x_train, dtype=np.float64)\n",
    "    x_test = np.array(x_test, dtype=np.float64)\n",
    "    \n",
    "    #reshaping\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "    \n",
    "    y_train = np.reshape(y_train, (y_train.shape[0],))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0],))\n",
    "        \n",
    "    #normalizing\n",
    "    mean_value = np.mean(x_train, axis=0)\n",
    "    x_train -= mean_value\n",
    "    x_test -= mean_value\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = preprocessing_data(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(10000, 3072)\n",
      "(50000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 너무 많아서 5000개랑 1000개만 사용해보도록 할게요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 확인\n",
    "\n",
    "실제 데이터가 어떻게 생겼는지 한번 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -71.71074,  -74.05614,  -69.5538 , ...,   -3.63908,  -33.8503 ,\n",
       "         -42.38186],\n",
       "       [  23.28926,   40.94386,   54.4462 , ...,   16.36092,    7.1497 ,\n",
       "          29.61814],\n",
       "       [ 124.28926,  118.94386,  122.4462 , ...,  -46.63908,  -39.8503 ,\n",
       "         -30.38186],\n",
       "       ...,\n",
       "       [  36.28926,   26.94386,   12.4462 , ...,  -84.63908,  -47.8503 ,\n",
       "         -30.38186],\n",
       "       [  23.28926,   15.94386,   -7.5538 , ...,   67.36092,  121.1497 ,\n",
       "          -0.38186],\n",
       "       [ -85.71074, -104.05614, -111.5538 , ...,   29.36092,   16.1497 ,\n",
       "         -14.38186]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 설정\n",
    "\n",
    "이제 하이퍼파라미터를 설정해볼게요.  \n",
    "hidden_size, epoch_size, batch_size, learning_rate등은 전부 하이퍼 파라미터이니 바꾸면서 도전해보세요!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "epoch_size = 1000\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "N = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 만들기\n",
    "\n",
    "input_size, hidden_size, output_size는 데이터에 맞게 잘 설정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mask = np.random.choice(N, batch_size) #이번 배치에서 쓸 데이터들 인덱스 추출\n",
    "x_batch = x_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = TwoLayerNet(x_batch, input_size=input_size, hidden_size=hidden_size, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy : 0.1\n",
      "0 test loss     : 2516.3799566632047\n",
      "10 test accuracy : 0.1\n",
      "10 test loss     : 2510.735660080766\n",
      "20 test accuracy : 0.1\n",
      "20 test loss     : 2485.5915747620734\n",
      "30 test accuracy : 0.131\n",
      "30 test loss     : 2394.3577949477535\n",
      "40 test accuracy : 0.126\n",
      "40 test loss     : 2331.444684463813\n",
      "50 test accuracy : 0.128\n",
      "50 test loss     : 2317.0705015234016\n",
      "60 test accuracy : 0.128\n",
      "60 test loss     : 2306.491943010696\n",
      "70 test accuracy : 0.127\n",
      "70 test loss     : 2297.5619865458098\n",
      "80 test accuracy : 0.127\n",
      "80 test loss     : 2290.9111185770003\n",
      "90 test accuracy : 0.125\n",
      "90 test loss     : 2285.6871294498796\n",
      "100 test accuracy : 0.124\n",
      "100 test loss     : 2281.5014737931688\n",
      "110 test accuracy : 0.126\n",
      "110 test loss     : 2277.5159646455904\n",
      "120 test accuracy : 0.127\n",
      "120 test loss     : 2275.232172617788\n",
      "130 test accuracy : 0.125\n",
      "130 test loss     : 2272.232833145006\n",
      "140 test accuracy : 0.125\n",
      "140 test loss     : 2268.1158841037177\n",
      "150 test accuracy : 0.123\n",
      "150 test loss     : 2263.3817315037163\n",
      "160 test accuracy : 0.124\n",
      "160 test loss     : 2257.8783347964663\n",
      "170 test accuracy : 0.127\n",
      "170 test loss     : 2249.4788111638154\n",
      "180 test accuracy : 0.131\n",
      "180 test loss     : 2236.986661289989\n",
      "190 test accuracy : 0.146\n",
      "190 test loss     : 2218.1975755098406\n",
      "200 test accuracy : 0.162\n",
      "200 test loss     : 2192.4756991738564\n",
      "210 test accuracy : 0.18\n",
      "210 test loss     : 2160.0772820147267\n",
      "220 test accuracy : 0.222\n",
      "220 test loss     : 2116.1520193461197\n",
      "230 test accuracy : 0.249\n",
      "230 test loss     : 2069.0942314239337\n",
      "240 test accuracy : 0.268\n",
      "240 test loss     : 2032.2520996462122\n",
      "250 test accuracy : 0.277\n",
      "250 test loss     : 2001.9643671672848\n",
      "260 test accuracy : 0.291\n",
      "260 test loss     : 1977.4538998792334\n",
      "270 test accuracy : 0.297\n",
      "270 test loss     : 1955.5408125735723\n",
      "280 test accuracy : 0.304\n",
      "280 test loss     : 1935.7631485927175\n",
      "290 test accuracy : 0.313\n",
      "290 test loss     : 1918.305677109296\n",
      "300 test accuracy : 0.316\n",
      "300 test loss     : 1902.9172463886837\n",
      "310 test accuracy : 0.322\n",
      "310 test loss     : 1890.100325479681\n",
      "320 test accuracy : 0.323\n",
      "320 test loss     : 1878.6594132433208\n",
      "330 test accuracy : 0.327\n",
      "330 test loss     : 1868.3797564738247\n",
      "340 test accuracy : 0.329\n",
      "340 test loss     : 1859.2245329544662\n",
      "350 test accuracy : 0.331\n",
      "350 test loss     : 1850.1067417653032\n",
      "360 test accuracy : 0.336\n",
      "360 test loss     : 1842.3637962799908\n",
      "370 test accuracy : 0.337\n",
      "370 test loss     : 1833.712687370882\n",
      "380 test accuracy : 0.345\n",
      "380 test loss     : 1826.7751172741835\n",
      "390 test accuracy : 0.346\n",
      "390 test loss     : 1818.515523839553\n",
      "400 test accuracy : 0.338\n",
      "400 test loss     : 1812.011645992936\n",
      "410 test accuracy : 0.34\n",
      "410 test loss     : 1804.712064417475\n",
      "420 test accuracy : 0.336\n",
      "420 test loss     : 1798.3832023607615\n",
      "430 test accuracy : 0.341\n",
      "430 test loss     : 1791.495711625577\n",
      "440 test accuracy : 0.341\n",
      "440 test loss     : 1785.1459347310904\n",
      "450 test accuracy : 0.346\n",
      "450 test loss     : 1778.961274569803\n",
      "460 test accuracy : 0.347\n",
      "460 test loss     : 1773.3387143720363\n",
      "470 test accuracy : 0.354\n",
      "470 test loss     : 1767.582703095923\n",
      "480 test accuracy : 0.36\n",
      "480 test loss     : 1761.5905416153819\n",
      "490 test accuracy : 0.367\n",
      "490 test loss     : 1756.50462373379\n",
      "500 test accuracy : 0.363\n",
      "500 test loss     : 1750.9546490262635\n",
      "510 test accuracy : 0.363\n",
      "510 test loss     : 1747.6755874488365\n",
      "520 test accuracy : 0.37\n",
      "520 test loss     : 1743.7906367601806\n",
      "530 test accuracy : 0.371\n",
      "530 test loss     : 1737.984782313693\n",
      "540 test accuracy : 0.372\n",
      "540 test loss     : 1734.2359281006652\n",
      "550 test accuracy : 0.372\n",
      "550 test loss     : 1730.7307173789052\n",
      "560 test accuracy : 0.377\n",
      "560 test loss     : 1726.9845884557599\n",
      "570 test accuracy : 0.376\n",
      "570 test loss     : 1723.646770797459\n",
      "580 test accuracy : 0.38\n",
      "580 test loss     : 1720.335556804104\n",
      "590 test accuracy : 0.381\n",
      "590 test loss     : 1718.122454266252\n",
      "600 test accuracy : 0.384\n",
      "600 test loss     : 1715.6378691421899\n",
      "610 test accuracy : 0.387\n",
      "610 test loss     : 1712.9384086024816\n",
      "620 test accuracy : 0.39\n",
      "620 test loss     : 1709.8293160325477\n",
      "630 test accuracy : 0.389\n",
      "630 test loss     : 1709.2956801718976\n",
      "640 test accuracy : 0.39\n",
      "640 test loss     : 1706.8909361012838\n",
      "650 test accuracy : 0.389\n",
      "650 test loss     : 1705.082712020507\n",
      "660 test accuracy : 0.386\n",
      "660 test loss     : 1701.4660352987116\n",
      "670 test accuracy : 0.387\n",
      "670 test loss     : 1698.877733834157\n",
      "680 test accuracy : 0.39\n",
      "680 test loss     : 1697.4731057653107\n",
      "690 test accuracy : 0.388\n",
      "690 test loss     : 1694.927994147685\n",
      "700 test accuracy : 0.391\n",
      "700 test loss     : 1691.9326695056004\n",
      "710 test accuracy : 0.39\n",
      "710 test loss     : 1691.6524469197295\n",
      "720 test accuracy : 0.392\n",
      "720 test loss     : 1688.7072849593183\n",
      "730 test accuracy : 0.394\n",
      "730 test loss     : 1690.0230245662392\n",
      "740 test accuracy : 0.391\n",
      "740 test loss     : 1688.5808075763935\n",
      "750 test accuracy : 0.395\n",
      "750 test loss     : 1686.8569348757994\n",
      "760 test accuracy : 0.396\n",
      "760 test loss     : 1686.3249165190948\n",
      "770 test accuracy : 0.392\n",
      "770 test loss     : 1686.415238935057\n",
      "780 test accuracy : 0.396\n",
      "780 test loss     : 1685.448561031452\n",
      "790 test accuracy : 0.394\n",
      "790 test loss     : 1684.7024205220243\n",
      "800 test accuracy : 0.398\n",
      "800 test loss     : 1682.5023861850075\n",
      "810 test accuracy : 0.403\n",
      "810 test loss     : 1683.2751242066417\n",
      "820 test accuracy : 0.403\n",
      "820 test loss     : 1682.7767061010709\n",
      "830 test accuracy : 0.403\n",
      "830 test loss     : 1681.6668748081725\n",
      "840 test accuracy : 0.402\n",
      "840 test loss     : 1682.196185052687\n",
      "850 test accuracy : 0.404\n",
      "850 test loss     : 1679.7688804417999\n",
      "860 test accuracy : 0.409\n",
      "860 test loss     : 1679.7017336522583\n",
      "870 test accuracy : 0.41\n",
      "870 test loss     : 1680.6517211018827\n",
      "880 test accuracy : 0.41\n",
      "880 test loss     : 1681.7228912426099\n"
     ]
    }
   ],
   "source": [
    "history = {'val_acc': [],'val_loss': []} #기록해서 그림 그리자!\n",
    "\n",
    "#코드를 보며 epoch, batch에 대해서 이해해봅시다.\n",
    "for i in range(epoch_size):\n",
    "    for j in range(N//batch_size):\n",
    "        batch_mask = np.random.choice(N, batch_size) #이번 배치에서 쓸 데이터들 인덱스 추출\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = y_train[batch_mask]\n",
    "        \n",
    "        nn.backward(x_batch, t_batch, 1e-7) # 가중치 갱신\n",
    "    \n",
    "    #accuracy와 loss를 기록해둡시다.\n",
    "    history[\"val_acc\"].append(nn.accuracy(x_test, y_test))\n",
    "    history[\"val_loss\"].append(nn.forward(x_test, y_test))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(i, \"test accuracy :\", nn.accuracy(x_test, y_test))\n",
    "        print(i, \"test loss     :\", nn.forward(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그림 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax_acc = fig.add_subplot(111)\n",
    "\n",
    "ax_acc.plot(range(epoch_size), history['val_acc'], label='정확도(%)', color='darkred')\n",
    "#plt.text(3, 14.7, \"<----------------정확도(%)\", verticalalignment='top', horizontalalignment='right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Validation Accuracy(%)')\n",
    "ax_acc.grid(linestyle='--', color='lavender')\n",
    "ax_loss = ax_acc.twinx()\n",
    "ax_loss.plot(range(epoch_size), history['val_loss'], label='오차', color='darkblue')\n",
    "#plt.text(3, 2.2, \"<----------------오차\", verticalalignment='top', horizontalalignment='left')\n",
    "plt.ylabel('Validation Error')\n",
    "ax_loss.yaxis.tick_right()\n",
    "ax_loss.grid(linestyle='--', color='lavender')\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()\n",
    "\n",
    "# 나의 최고 validation accuracy는? 두구두구~\n",
    "print(\"나의 최고 validation loss : \",max(history['val_acc']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
