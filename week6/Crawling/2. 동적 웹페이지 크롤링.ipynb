{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인스타그램 검색어에 대한 해시태그 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 참고: https://github.com/Yeowoolee/Instagram-crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import selenium.webdriver as webdriver\n",
    "from urllib.request import Request, urlopen\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = 'bubble_tea' # 검색어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.instagram.com/explore/tags/'+str(search)+'/' # 검색어에 대한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('./chromedriver.exe') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)\n",
    "sleep(5) #로딩 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCROLL_PAUSE_TIME = 1.5 #로딩 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "linklist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    pageString = driver.page_source\n",
    "    bsObj = BeautifulSoup(pageString, \"lxml\")\n",
    "\n",
    "    for link1 in bsObj.find_all(name=\"div\",attrs={\"class\":\"Nnq7C weEfm\"}):\n",
    "\n",
    "            title = link1.select('a')[0] \n",
    "            link = title.attrs['href']\n",
    "            linklist.append(link) \n",
    "            title = link1.select('a')[1] \n",
    "            link = title.attrs['href']\n",
    "            linklist.append(link) \n",
    "            title = link1.select('a')[2] \n",
    "            link = title.attrs['href']\n",
    "            linklist.append(link)\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    sleep(SCROLL_PAUSE_TIME)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    if new_height == last_height:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            last_height = new_height\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8262"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(linklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 너무 길어서 시간이 오래 걸려서 일단 10개만 합니다\n",
    "ten = linklist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_and_hashtag_to_csv(linklist, search):\n",
    "    df = [['insta_account', 'hashtag']]\n",
    "    for l in linklist:\n",
    "        rows = []\n",
    "        \n",
    "        req = Request('https://www.instagram.com/p'+l,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage,\"lxml\",from_encoding='utf-8')\n",
    "        soup1 = soup.find(\"meta\",attrs={\"property\":\"og:description\"})\n",
    "        \n",
    "        insta_account = soup1['content']\n",
    "        insta_account = insta_account[insta_account.find(\"@\")+1:insta_account.find(\")\")]\n",
    "        insta_account = insta_account[:20]\n",
    "        if insta_account == '':\n",
    "            continue\n",
    "        \n",
    "        for hashtag in soup.find_all(\"meta\",attrs={\"property\":\"instapp:hashtags\"}):\n",
    "            hashtag = hashtag['content']\n",
    "            hashtag = hashtag[:10]\n",
    "            \n",
    "            rows.append([insta_account, hashtag])\n",
    "        \n",
    "        df.extend(rows)\n",
    "    \n",
    "    data = pd.DataFrame(df)\n",
    "    data.to_csv('{}.csv'.format(search), mode='w',encoding='utf-8',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_and_hashtag_to_csv(ten, search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_and_hashtag_to_json(linklist, search):\n",
    "    dt = dict()\n",
    "    \n",
    "    for l in linklist:\n",
    "        req = Request('https://www.instagram.com/p'+l,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage,\"lxml\",from_encoding='utf-8')\n",
    "        soup1 = soup.find(\"meta\",attrs={\"property\":\"og:description\"})\n",
    "        \n",
    "        insta_account = soup1['content']\n",
    "        insta_account = insta_account[insta_account.find(\"@\")+1:insta_account.find(\")\")]\n",
    "        insta_account = insta_account[:20]\n",
    "        if insta_account == '':\n",
    "            continue\n",
    "        \n",
    "        if insta_account in dt.keys():\n",
    "            pass\n",
    "        else:\n",
    "            dt[insta_account] = []\n",
    "            \n",
    "        for hashtag in soup.find_all(\"meta\",attrs={\"property\":\"instapp:hashtags\"}):\n",
    "            hashtag = hashtag['content']\n",
    "            hashtag = hashtag[:10]\n",
    "            dt[insta_account].append(hashtag)\n",
    "    \n",
    "    with open('{}.json'.format(search),'w') as fp:\n",
    "        json.dump(dt, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_and_hashtag_to_json(ten, search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브라우저 끄기\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
