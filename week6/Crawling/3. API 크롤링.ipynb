{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit 크롤러"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pushshift API 사용\n",
    "- 출처: https://github.com/S00ahKim/Reddit-Submissions-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_time(tmstmp, hour):\n",
    "    '''\n",
    "    시간 계산기: 타임스탬프에 주어진 인자만큼의 시간을 더한 타임스탬프를 리턴\n",
    "    '''\n",
    "    tmp = int(tmstmp)\n",
    "    dttime = datetime.fromtimestamp(tmp)\n",
    "    wttime = dttime + dt.timedelta(hours = hour)\n",
    "    return str(int(wttime.timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_server_time():\n",
    "    '''\n",
    "    reddit 서버 시간 구하기\n",
    "    '''\n",
    "    date = urllib.request.urlopen('http://www.google.com').headers['Date']\n",
    "    timestmp = int(time.mktime(time.strptime(date, '%a, %d %b %Y %H:%M:%S %Z')))\n",
    "    return timestmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_roman(x):\n",
    "    '''\n",
    "    인코딩할 수 없는 문자열 제거\n",
    "    '''\n",
    "    return str(x).encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_end_by_start(start):\n",
    "    '''\n",
    "    시작 시간을 인자로 주면 한달 뒤의 시간을 문자열로 리턴한다.\n",
    "    '''\n",
    "    tmp = int(start)\n",
    "    dttime = datetime.fromtimestamp(tmp)\n",
    "    wttime = dttime + relativedelta(months=1)\n",
    "    return str(int(wttime.timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_time(subreddit):\n",
    "    meta = pd.read_csv('./data/metadata/data.csv', header=0)\n",
    "    try:\n",
    "        start = meta.loc[meta['subreddit'] == subreddit, 'last'].values[0]\n",
    "    except:\n",
    "        r = requests.get('https://www.reddit.com/r/{}/about.json'.format(subreddit), headers = {'User-agent': 'reddit_crawler'}).json()\n",
    "        start = int(r['data']['created_utc'])\n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_meta(subreddit, last):\n",
    "    meta = pd.read_csv('./data/metadata/data.csv', header=0)\n",
    "    try:\n",
    "        meta.loc[meta['subreddit'] == subreddit, 'last'] = last\n",
    "        meta.to_csv('./data/metadata/data.csv', index=False)\n",
    "    except:\n",
    "        meta = meta.append({'subreddit':subreddit, 'last':last}, ignore_index=True)\n",
    "        meta = meta[['subreddit', 'last']]\n",
    "        meta.to_csv('./data/metadata/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit(subreddit, end_time):\n",
    "    start_time = get_start_time(subreddit)\n",
    "\n",
    "    arr = []\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&after={start}&before={end}&sort=asc'\n",
    "    get_all = False\n",
    "\n",
    "    last_get_time = 0\n",
    "\n",
    "    while not get_all:\n",
    "        next_time = cal_time(start_time, 10)\n",
    "        url = base_url.format(subreddit=subreddit, start=start_time, end=next_time)\n",
    "        try:\n",
    "            r = requests.get(url).json()\n",
    "        except:\n",
    "            r = {'data':[]}\n",
    "            print(requests.get(url))\n",
    "\n",
    "        if len(r['data']) == 0:\n",
    "            if int(last_get_time) >= int(end_time):\n",
    "                get_all = True\n",
    "            else:\n",
    "                start_time = next_time\n",
    "                next_time = cal_time(start_time, 10)\n",
    "        \n",
    "        else:\n",
    "            for submission in r['data']:\n",
    "                # 날짜 분리\n",
    "                timestamp = int(submission['created_utc'])\n",
    "                d = datetime.fromtimestamp(timestamp)\n",
    "                date = str(d.year)+'-'+str(d.month)+'-'+str(d.day)\n",
    "\n",
    "                # 문자열에 대해 인코딩 처리\n",
    "                sbr = encode_roman(submission['subreddit'])\n",
    "                sbr_id = encode_roman(submission['subreddit_id'])\n",
    "                uid = encode_roman(submission['id'])\n",
    "                domain = encode_roman(submission['domain'])\n",
    "                title = encode_roman(submission['title'])\n",
    "                full_link = encode_roman(submission['full_link'])\n",
    "                author = encode_roman(submission['author'])\n",
    "                try:\n",
    "                    if submission['selftext']:\n",
    "                        selftext = encode_roman(submission['selftext'])\n",
    "                    else:\n",
    "                        selftext = ''\n",
    "                except:\n",
    "                    selftext = ''\n",
    "\n",
    "                try:\n",
    "                    if submission['thumbnail']:\n",
    "                        thumbnail = encode_roman(submission['thumbnail'])\n",
    "                    else:\n",
    "                        thumbnail = ''\n",
    "                except:\n",
    "                    thumbnail = ''\n",
    "\n",
    "                try:\n",
    "                    if submission['media']['oembed']:\n",
    "                        for e in submission['media']['oembed']:\n",
    "                            media_provider = encode_roman(e['provider_name'])\n",
    "                            media_thumbnail = encode_roman(e['thumbnail_url'])\n",
    "                            media_title = encode_roman(e['title'])\n",
    "                            media_description = encode_roman(e['description'])\n",
    "                            media_url = encode_roman(e['url'])\n",
    "                            break\n",
    "                except:\n",
    "                    media_provider = ''\n",
    "                    media_thumbnail = ''\n",
    "                    media_title = ''\n",
    "                    media_description = ''\n",
    "                    media_url = ''\n",
    "\n",
    "                over_18 = str(submission['over_18'])\n",
    "\n",
    "                # 데이터 저장\n",
    "                if len(arr)>1 and d.month != arr[-1][4]:\n",
    "                    get_all = True\n",
    "                else:\n",
    "                    tmp = [sbr, sbr_id, date, d.year, d.month, d.day, d.hour, uid, domain, title, full_link, \n",
    "                            author, submission['created_utc'], selftext, submission['num_comments'], \n",
    "                            submission['score'], over_18, thumbnail, media_provider,\n",
    "                            media_thumbnail, media_title, media_description, media_url]\n",
    "                    arr.append(tmp)\n",
    "\n",
    "                # 마지막 날짜 기록용\n",
    "                last_get_time = int(submission['created_utc'])\n",
    "\n",
    "            start_time = last_get_time\n",
    "            next_time = cal_time(start_time, 10)\n",
    "\n",
    "    df_save = pd.DataFrame(arr)\n",
    "    df_save.columns = ['sbr', 'sbr_id', 'date', 'year', 'month', 'hour', 'day', 'id', \n",
    "                        'domain', 'title', 'full_link', 'author', 'created', 'selftext', \n",
    "                        'num_comments', 'score', 'over_18', 'thumbnail', 'media_provider',\n",
    "                        'media_thumbnail', 'media_title', 'media_description', 'media_url']\n",
    "\n",
    "\n",
    "    dtm = datetime.fromtimestamp(last_get_time)\n",
    "    yy = str(dtm.year)\n",
    "    mm = str(dtm.month)\n",
    "\n",
    "    if len(mm) < 2:\n",
    "        mm = '0'+mm\n",
    "        \n",
    "    path = './data/scrapped/{dirname}/'.format(dirname = subreddit)\n",
    "    save_file_name = path + '{yy}-{mm}-{last}.csv'.format(yy=yy, mm=mm, last=last_get_time)\n",
    "    df_save.to_csv(save_file_name, encoding='cp949')\n",
    "    modify_meta(subreddit, last_get_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    os.chdir('./data/scrapped/')\n",
    "    all_dirs = [os.path.abspath(name) for name in os.listdir(\".\") if os.path.isdir(name)]\n",
    "    end_time = get_server_time()\n",
    "    print(end_time)\n",
    "    for dirc in all_dirs:\n",
    "        subreddit = os.path.basename(os.path.normpath(dirc))\n",
    "        print('crawling subreddit... ', subreddit)\n",
    "        get_reddit(subreddit, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API를 사용하는 크롤러 코드 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 공공데이터 API를 사용하는 방법, 일반적으로 API로 크롤링할 때의 방법을 잘 설명해 둠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[오픈 API를 통한 공공데이터 수집](https://medium.com/@whj2013123218/%EC%98%A4%ED%94%88-api%EB%A5%BC-%ED%86%B5%ED%95%9C-%EA%B3%B5%EA%B3%B5%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91-e1dd0ad203b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 크롤링을 하면서 고려하게 되는 것들을 예시와 함께 설명해 둠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[네이버 API를 사용한 검색어 트렌드 크롤링 툴](https://brunch.co.kr/@sukhyun9673/13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 그리고...\n",
    "- 네이버, github, 트위터 등 크롤링 대상이 될 만한 데이터들이 많은 사이트에서는 대체로 API를 제공하고 있다.\n",
    "- github에서는 완성된 크롤러 코드를 많이 볼 수 있다 (star 개수가 많은 레포지토리를 추천!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
