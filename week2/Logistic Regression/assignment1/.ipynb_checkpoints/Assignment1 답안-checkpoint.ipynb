{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1 예시 답안\n",
    "데이터에 대한 추가 설명을 따로 주지 않았던 탓에 어떤 metrics가 타당할지에 대한 생각을 하는 데에 어려움이 있을 수 있다고 판단했습니다.   \n",
    "이에 데이터 출처를 따로 설명하고 credit card fraud detection의 문제임을 밝힐 예정입니다.    \n",
    "따라서 해당 데이터에 가장 적합한 성능 지표를 찾고 이와 Accuracy 등을 비교하여 설명하는 과제 제출자가 우수 과제 선정 가능성이 높습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sampled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    28428\n",
       "1       52\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts() ## imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "      <td>28480.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94643.457198</td>\n",
       "      <td>-0.009853</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.023257</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>-0.002983</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>-0.001175</td>\n",
       "      <td>-0.002359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>0.005655</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.003401</td>\n",
       "      <td>-0.002118</td>\n",
       "      <td>-0.003817</td>\n",
       "      <td>88.245961</td>\n",
       "      <td>0.001826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47588.652617</td>\n",
       "      <td>1.921643</td>\n",
       "      <td>1.636421</td>\n",
       "      <td>1.485803</td>\n",
       "      <td>1.419452</td>\n",
       "      <td>1.353022</td>\n",
       "      <td>1.321492</td>\n",
       "      <td>1.190563</td>\n",
       "      <td>1.180562</td>\n",
       "      <td>1.092514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740709</td>\n",
       "      <td>0.721913</td>\n",
       "      <td>0.569450</td>\n",
       "      <td>0.610268</td>\n",
       "      <td>0.515988</td>\n",
       "      <td>0.484652</td>\n",
       "      <td>0.395407</td>\n",
       "      <td>0.304311</td>\n",
       "      <td>235.775903</td>\n",
       "      <td>0.042692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>-33.669917</td>\n",
       "      <td>-47.429676</td>\n",
       "      <td>-25.188773</td>\n",
       "      <td>-5.600607</td>\n",
       "      <td>-25.025820</td>\n",
       "      <td>-20.054615</td>\n",
       "      <td>-28.011293</td>\n",
       "      <td>-37.353443</td>\n",
       "      <td>-8.960922</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.790142</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-14.767079</td>\n",
       "      <td>-2.814898</td>\n",
       "      <td>-3.768740</td>\n",
       "      <td>-1.658162</td>\n",
       "      <td>-9.793568</td>\n",
       "      <td>-8.656570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54244.000000</td>\n",
       "      <td>-0.948748</td>\n",
       "      <td>-0.590151</td>\n",
       "      <td>-0.850020</td>\n",
       "      <td>-0.842293</td>\n",
       "      <td>-0.696015</td>\n",
       "      <td>-0.766332</td>\n",
       "      <td>-0.551560</td>\n",
       "      <td>-0.205536</td>\n",
       "      <td>-0.640165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226194</td>\n",
       "      <td>-0.532159</td>\n",
       "      <td>-0.160307</td>\n",
       "      <td>-0.354593</td>\n",
       "      <td>-0.316866</td>\n",
       "      <td>-0.329481</td>\n",
       "      <td>-0.071556</td>\n",
       "      <td>-0.053896</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84099.500000</td>\n",
       "      <td>-0.004241</td>\n",
       "      <td>0.068011</td>\n",
       "      <td>0.207794</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>-0.057630</td>\n",
       "      <td>-0.269471</td>\n",
       "      <td>0.046837</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>-0.053404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029125</td>\n",
       "      <td>0.009972</td>\n",
       "      <td>-0.009117</td>\n",
       "      <td>0.040856</td>\n",
       "      <td>0.013365</td>\n",
       "      <td>-0.060068</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.011243</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139340.250000</td>\n",
       "      <td>1.307201</td>\n",
       "      <td>0.799815</td>\n",
       "      <td>1.040033</td>\n",
       "      <td>0.751093</td>\n",
       "      <td>0.610578</td>\n",
       "      <td>0.401779</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.327867</td>\n",
       "      <td>0.588938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186376</td>\n",
       "      <td>0.522956</td>\n",
       "      <td>0.148473</td>\n",
       "      <td>0.438138</td>\n",
       "      <td>0.349229</td>\n",
       "      <td>0.236058</td>\n",
       "      <td>0.089146</td>\n",
       "      <td>0.077542</td>\n",
       "      <td>77.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172785.000000</td>\n",
       "      <td>2.439207</td>\n",
       "      <td>14.706335</td>\n",
       "      <td>4.079168</td>\n",
       "      <td>13.143668</td>\n",
       "      <td>29.016124</td>\n",
       "      <td>14.793318</td>\n",
       "      <td>26.237722</td>\n",
       "      <td>15.374630</td>\n",
       "      <td>8.933028</td>\n",
       "      <td>...</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>4.725713</td>\n",
       "      <td>17.606637</td>\n",
       "      <td>3.563214</td>\n",
       "      <td>3.561216</td>\n",
       "      <td>2.992947</td>\n",
       "      <td>6.250240</td>\n",
       "      <td>15.726807</td>\n",
       "      <td>7541.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count   28480.000000  28480.000000  28480.000000  28480.000000  28480.000000   \n",
       "mean    94643.457198     -0.009853      0.002283      0.023257      0.007209   \n",
       "std     47588.652617      1.921643      1.636421      1.485803      1.419452   \n",
       "min        12.000000    -33.669917    -47.429676    -25.188773     -5.600607   \n",
       "25%     54244.000000     -0.948748     -0.590151     -0.850020     -0.842293   \n",
       "50%     84099.500000     -0.004241      0.068011      0.207794     -0.005891   \n",
       "75%    139340.250000      1.307201      0.799815      1.040033      0.751093   \n",
       "max    172785.000000      2.439207     14.706335      4.079168     13.143668   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  28480.000000  28480.000000  28480.000000  28480.000000  28480.000000   \n",
       "mean      -0.002983      0.001408      0.001781     -0.001175     -0.002359   \n",
       "std        1.353022      1.321492      1.190563      1.180562      1.092514   \n",
       "min      -25.025820    -20.054615    -28.011293    -37.353443     -8.960922   \n",
       "25%       -0.696015     -0.766332     -0.551560     -0.205536     -0.640165   \n",
       "50%       -0.057630     -0.269471      0.046837      0.020862     -0.053404   \n",
       "75%        0.610578      0.401779      0.571420      0.327867      0.588938   \n",
       "max       29.016124     14.793318     26.237722     15.374630      8.933028   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  28480.000000  28480.000000  28480.000000  28480.000000   \n",
       "mean   ...      0.001560      0.001996      0.005655      0.000389   \n",
       "std    ...      0.740709      0.721913      0.569450      0.610268   \n",
       "min    ...    -15.790142     -8.887017    -14.767079     -2.814898   \n",
       "25%    ...     -0.226194     -0.532159     -0.160307     -0.354593   \n",
       "50%    ...     -0.029125      0.009972     -0.009117      0.040856   \n",
       "75%    ...      0.186376      0.522956      0.148473      0.438138   \n",
       "max    ...     27.202839      4.725713     17.606637      3.563214   \n",
       "\n",
       "                V25           V26           V27           V28        Amount  \\\n",
       "count  28480.000000  28480.000000  28480.000000  28480.000000  28480.000000   \n",
       "mean       0.000116     -0.003401     -0.002118     -0.003817     88.245961   \n",
       "std        0.515988      0.484652      0.395407      0.304311    235.775903   \n",
       "min       -3.768740     -1.658162     -9.793568     -8.656570      0.000000   \n",
       "25%       -0.316866     -0.329481     -0.071556     -0.053896      5.750000   \n",
       "50%        0.013365     -0.060068      0.000948      0.011243     21.980000   \n",
       "75%        0.349229      0.236058      0.089146      0.077542     77.200000   \n",
       "max        3.561216      2.992947      6.250240     15.726807   7541.700000   \n",
       "\n",
       "              Class  \n",
       "count  28480.000000  \n",
       "mean       0.001826  \n",
       "std        0.042692  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe() # Scaling을 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y 구분\n",
    "X = data.drop('Class', axis = 1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 X: (21360, 30), 학습데이터 y: (21360,), 테스트데이터 X: (7120, 30), 테스트데이터 y: (7120,)\n"
     ]
    }
   ],
   "source": [
    "print(\"학습데이터 X: {}, 학습데이터 y: {}, 테스트데이터 X: {}, 테스트데이터 y: {}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.14108787,  0.58692535, -0.96859154, ...,  0.10758963,\n",
       "         0.15818377,  0.29765166],\n",
       "       [ 1.45624903,  1.08104822,  0.06234035, ..., -0.01913339,\n",
       "        -0.18118879, -0.36883746],\n",
       "       [ 0.75833894,  0.95436709, -0.60243691, ...,  0.28458186,\n",
       "        -0.08727433, -0.10139278],\n",
       "       ...,\n",
       "       [-1.17848378, -0.39306929,  0.3675534 , ..., -0.29358649,\n",
       "        -0.26392097, -0.06106382],\n",
       "       [ 1.56942307, -0.22082572,  0.14591662, ...,  0.0765422 ,\n",
       "        -0.00476937, -0.28817954],\n",
       "       [ 1.57928619,  0.98278649, -0.10310982, ..., -0.1759003 ,\n",
       "        -0.0853167 , -0.15891461]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit_transform(X_train)\n",
    "ss.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 적합\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998876404494382"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test,y_test) # accuracy가 굉장히 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7108,    3],\n",
       "       [   5,    4]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix 찍어보기\n",
    "y_pred = logreg.predict(X_test)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn의 confusion matrix는   \n",
    "(0,0) : TN  \n",
    "(0,1): FP  \n",
    "(1,0): FN  \n",
    "(1,1): TN\n",
    "\n",
    "즉, logistic regression의 경우 confusion matrix에서 실제 0이지만 1이라고 예측한 경우 3개,    \n",
    "confusion matrix에서 실제 1이지만 0이라고 예측한 경우 1개가 있음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall을 찍어보자. (fraud detection의 경우 recall이 가장 적합한 성능 지표)\n",
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확실히 Accuracy에 비해 많이 떨어짐을 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Cut-off 조절\n",
    "최적의 cut-off를 찾기 위해 ROC_curve를 이용할 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_score = logreg.predict_proba(X_test)[:,1] # Class = 1일 확률의 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.90892779e-04, 2.61287000e-04, 1.42048846e-06, ...,\n",
       "       3.65805778e-03, 6.04186590e-04, 4.21885751e-04])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, pred_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Sensitivity(TPR)')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAXb0lEQVR4nO3dfbRddX3n8ffHIFgLUWvSijwYrKCN1oK95aFUxWItsCqZGR0FZQSHinYGtaCuRatFF62zqJaiLnEwAxaqIqB2JLVh4SPqKESCAkIcnAwgBJhFUAZ0VBD8zh9nB09PTu45Se4+J/fu92utu+5++J1zvjsP93P3b+/9+6WqkCR112OmXYAkaboMAknqOINAkjrOIJCkjjMIJKnjdpp2AVtryZIltWzZsmmXIUnzyrXXXntvVS0dtm/eBcGyZctYu3bttMuQpHklyfe3tM+uIUnqOINAkjrOIJCkjjMIJKnjDAJJ6rjWgiDJR5Lck+TGLexPkg8kWZ/khiTPa6sWSdKWtXlGcAFwxCz7jwT2bb5OAv5ri7VIkragtecIquqrSZbN0mQF8I/VGwf76iRPTLJ7Vd3dVk2jXLTmdi677s5pfbwkzWr5Uxfzzpc+e87fd5rXCPYA7uhb39Bs20ySk5KsTbJ248aNrRV02XV3su7uB1p7f0naEU3zyeIM2TZ0lpyqWgmsBJiZmWl1Jp3luy/mktcf0uZHSNIOZZpnBBuAvfrW9wTumlItktRZ0wyCVcBrmruHDgbun+b1AUnqqta6hpJ8AjgMWJJkA/BO4LEAVXUusBo4ClgP/AR4bVu1SJK2rM27ho4dsb+A/9zW50uSxuOTxZLUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgSR1nEEgSR1nEEhSxxkEktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHTfNOYun5qI1t3PZdXdutn3d3Q+wfPfFU6hIkqank2cEl113J+vufmCz7ct3X8yK/feYQkWSND2dPCOA3g/9S15/yLTLkKSp6+QZgSTplwwCSeo4g0CSOs4gkKSOMwgkqeMMAknqOINAkjrOIJCkjjMIJKnjDAJJ6jiDQJI6ziCQpI4zCCSp41oNgiRHJLk5yfokpw3Zv3eSLyf5dpIbkhzVZj2SpM21FgRJFgHnAEcCy4FjkywfaPYO4NKqOgA4BvhQW/VIkoZr84zgQGB9Vd1SVQ8BFwMrBtoUsGlKsCcAd7VYjyRpiDaDYA/gjr71Dc22fu8CjkuyAVgNvHHYGyU5KcnaJGs3btzYRq2S1FltBkGGbKuB9WOBC6pqT+Ao4KNJNqupqlZW1UxVzSxdurSFUiWpu9oMgg3AXn3re7J518+JwKUAVXUV8DhgSYs1SZIGtBkE1wD7Jtknyc70LgavGmhzO3A4QJLfohcErfX9XLTmdl754auGTlwvSV3VWhBU1cPAycAVwHfp3R10U5IzkhzdNHsL8Lok1wOfAE6oqsHuozlz2XV3su7uB1i++2JW7D94uUKSummnNt+8qlbTuwjcv+30vuV1wKFt1jBo+e6LueT1h0zyIyVph+aTxZLUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgSR1nEEgSR1nEEhSxxkEktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHWcQSFLHGQSS1HEjZyhLcghwHPB8YHfgp8CNwL8AH6uq+1utUJLUqlnPCJJcDvwpvXmHj6AXBMuBd9CbaP6yvvmHJUnz0Kgzgv9QVfcObPsx8K3m66wkS1qpTJI0EbOeEQwJAQCSLEry6tnaSJLmh1FdQ4uT/EWSDyZ5SXreCNwCvGIyJUqS2jSqa+ijwH3AVfSuFbwN2BlYUVXXtVybJGkCRgXB06vqtwGSnAfcC+xdVT9qvTJJ0kSMeo7g55sWquoR4FZDQJIWllFnBL+T5AEgzfqv9K1XVS1utTpJUutmDYKqWjSpQiRJ0zFrECR5HPAG4BnADcBHqurhSRQmSZqMUdcILgRmgO8ARwFntV6RJGmiRl0jWN5319D5wDfbL0mSNElbc9fQVncJJTkiyc1J1ic5bQttXpFkXZKbkly0tZ8hSdo+4941BL07hca+ayjJIuAc4I+ADcA1SVZV1bq+NvsCfwEcWlX3Jfn17TgWSdI2GBUEj6uqn49osyUHAuur6haAJBcDK4B1fW1eB5xTVfcBVNU92/hZkqRtNKpraM12vPcewB196xuabf32A/ZL8vUkVyc5YtgbJTkpydokazdu3LgdJUmSBo0KgozYv7WvrYH1nYB9gcOAY4HzkjxxsxdVrayqmaqaWbp06XaUJEkaNKpraGmSU7e0s6r+fpbXbgD26lvfE7hrSJurm+6nW5PcTC8YrhlRlyRpjow6I1gE7ArstoWv2VwD7JtknyQ7A8cAqwbafAZ4EUAzwc1+9Ia4liRNyKgzgrur6oxteeOqejjJyfSmuVxE76nkm5KcAaytqlXNvpckWQc8Arytqn6wLZ8nSdo2o4Jge64RUFWrgdUD207vWy7g1OZLkjQFo7qGRk5Mn2TXOapFkjQFo4LggiRnJXlBkl/dtDHJ05OcmOQKYOgtn5Kk+WHUMNSHJzkKeD1waJInAQ8DNwP/AhxfVf+n/TIlSW0ZdY1gaD+/JGnhGNU1BECSTyU5KslY7SVJ88e4P9jPBV4N/K8kZyZ5Vos1SZImaKwgqKovVNWrgecBtwGfT/KNJK9N8tg2C5QktWvsrp4kTwZOAP4U+DbwfnrB8PlWKpMkTcTIi8UASf4JeBbwUeClVXV3s+uSJGvbKk6S1L6xggA4r7l76FFJdqmqB6tqpoW6JEkTMm7X0N8M2XbVXBYiSZqOWc8IkjyF3mQyv5LkAH459tBi4PEt1yZJmoBRXUN/TO8C8Z5A/9wDPwL+sqWaJEkTNGqIiQuBC5O8rKo+PaGaJEkTNKpr6Liq+hiwbNhMZSNmKJMkzQOjuoY2jTjqUNOStECN6hr6cLP4oaraOIF6JEkTNu7to99I8rlmDoIntVqRJGmixh1raF/gHcCzgWuTfDbJca1WJkmaiLHHGqqqb1bVqcCBwA+BC1urSpI0MePOR7A4yfFJLge+AdxNLxAkSfPcuGMNXQ98BjijqhxaQpIWkHGD4OlVVa1WIkmailEPlL2vqv4cWJVksyCoqqNbq0ySNBGjzgg+2nz/u7YLadtFa25nza0/5KB9fm3apUjSDmXUA2XXNov7V9X7+/cleTPwlbYKm2uXXXcnACv232PKlUjSjmXc20ePH7LthDmsYyIO2ufXeNVBe0+7DEnaoYy6RnAs8CpgnySr+nbtBvygzcIkSZMx6hrBpmcGlgBn9W3/EXBDW0VJkiZn1DWC7wPfBw6ZTDmSpEkb1TX0P6rqD5L8COi/fTRAVdXiVquTJLVu1ovFVfUHzffdqmpx39du44RAkiOS3JxkfZLTZmn38iSVZGbrD0GStD3GHWvoN5Ps0iwfluRNSZ444jWLgHOAI4HlwLFJlg9ptxvwJmDN1hYvSdp+494++mngkSTPAM4H9gEuGvGaA4H1VXVLVT0EXAysGNLur4H3AD8bsxZJ0hwaNwh+UVUPA/8WeF9VnQLsPuI1ewB39K1vaLY9KskBwF5V9dnZ3ijJSUnWJlm7caMTpUnSXBo3CH7ePFNwPLDph/ZjR7wmQ7Y9esE5yWOAs4G3jPrwqlpZVTNVNbN06dIxS5YkjWPcIHgtvVtI311VtybZB/jYiNdsAPbqW98TuKtvfTfgOcCVSW4DDqY3uJ0XjCVpgsYahrqq1tG7oLtp/VbgzBEvuwbYtwmNO4Fj6D2lvOk97qf3oBoASa4E3lpVa8ctXpK0/ca9a+jQJJ9P8r0ktyS5Nckts72muaZwMnAF8F3g0qq6KckZSRy+WpJ2EONOTHM+cApwLfDIuG9eVauB1QPbTt9C28PGfV9J0twZNwjur6rLW61EkjQV4wbBl5O8F/gn4MFNG6vqW61UJUmamHGD4KDme/8dPQX84dyWI0matHHvGnpR24VIkqZj3LuGfiPJ+Ukub9aXJzmx3dIkSZMw7gNlF9C7DfSpzfr3gD9voyBJ0mSNGwRLqupS4Bfw6DMCY99GKknacY0bBP8vyZNpxgpKcjBwf2tVSZImZty7hk4FVgG/meTrwFLg5a1VJUmamFnPCJL8XpKnNM8LvBD4S3rPEXyO3qBykqR5blTX0IeBh5rl3wfeTm/WsfuAlS3WJUmakFFdQ4uq6ofN8iuBlVX1aeDTSa5rtzRJ0iSMOiNYlGRTWBwOfKlv37jXFyRJO7BRP8w/AXwlyb3AT4GvATRzF3vXkCQtALMGQVW9O8kX6c1P/Lmq2jTV5GOAN7ZdnCSpfSO7d6rq6iHbvtdOOZKkSRv3gTJJ0gJlEEhSxxkEktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgSR1nEEgSR1nEEhSx7UaBEmOSHJzkvVJThuy/9Qk65LckOSLSZ7WZj2SpM21FgRJFtGb6P5IYDlwbJLlA82+DcxU1XOBTwHvaaseSdJwbZ4RHAisr6pbquoh4GJgRX+DqvpyVf2kWb0a2LPFeiRJQ7QZBHsAd/Stb2i2bcmJwOXDdiQ5KcnaJGs3btw4hyVKktoMggzZVkO2keQ4YAZ477D9VbWyqmaqambp0qVzWKIkaeScxdthA7BX3/qewF2DjZK8GHg78MKqerDFeiRJQ7R5RnANsG+SfZLsDBwDrOpvkOQA4MPA0VV1T4u1SJK2oLUgqKqHgZOBK4DvApdW1U1JzkhydNPsvcCuwCeTXJdk1RbeTpLUkja7hqiq1cDqgW2n9y2/uM3PlySN5pPFktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgSR1nEEgSR1nEEhSxxkEktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgSR1nEEgSR1nEEhSxxkEktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHddqECQ5IsnNSdYnOW3I/l2SXNLsX5NkWZv1SJI211oQJFkEnAMcCSwHjk2yfKDZicB9VfUM4Gzgb9uqR5I0XJtnBAcC66vqlqp6CLgYWDHQZgVwYbP8KeDwJGmxJknSgJ1afO89gDv61jcAB22pTVU9nOR+4MnAvf2NkpwEnASw9957b1Mxy5+6eJteJ0kLXZtBMOw3+9qGNlTVSmAlwMzMzGb7x/HOlz57W14mSQtem11DG4C9+tb3BO7aUpskOwFPAH7YYk2SpAFtBsE1wL5J9kmyM3AMsGqgzSrg+Gb55cCXqmqbfuOXJG2b1rqGmj7/k4ErgEXAR6rqpiRnAGurahVwPvDRJOvpnQkc01Y9kqTh2rxGQFWtBlYPbDu9b/lnwL9vswZJ0ux8sliSOs4gkKSOMwgkqeMMAknquMy3uzWTbAS+v40vX8LAU8sd4DF3g8fcDdtzzE+rqqXDdsy7INgeSdZW1cy065gkj7kbPOZuaOuY7RqSpI4zCCSp47oWBCunXcAUeMzd4DF3QyvH3KlrBJKkzXXtjECSNMAgkKSOW5BBkOSIJDcnWZ/ktCH7d0lySbN/TZJlk69ybo1xzKcmWZfkhiRfTPK0adQ5l0Ydc1+7lyepJPP+VsNxjjnJK5q/65uSXDTpGufaGP+2907y5STfbv59HzWNOudKko8kuSfJjVvYnyQfaP48bkjyvO3+0KpaUF/0hrz+38DTgZ2B64HlA23+E3Bus3wMcMm0657AMb8IeHyz/GddOOam3W7AV4GrgZlp1z2Bv+d9gW8DT2rWf33adU/gmFcCf9YsLwdum3bd23nMLwCeB9y4hf1HAZfTm+HxYGDN9n7mQjwjOBBYX1W3VNVDwMXAioE2K4ALm+VPAYcnGTZt5nwx8pir6stV9ZNm9Wp6M8bNZ+P8PQP8NfAe4GeTLK4l4xzz64Bzquo+gKq6Z8I1zrVxjrmATZOSP4HNZ0KcV6rqq8w+U+MK4B+r52rgiUl2357PXIhBsAdwR9/6hmbb0DZV9TBwP/DkiVTXjnGOud+J9H6jmM9GHnOSA4C9quqzkyysReP8Pe8H7Jfk60muTnLExKprxzjH/C7guCQb6M1/8sbJlDY1W/v/faRWJ6aZkmG/2Q/eIztOm/lk7ONJchwwA7yw1YraN+sxJ3kMcDZwwqQKmoBx/p53otc9dBi9s76vJXlOVf3flmtryzjHfCxwQVWdleQQerMePqeqftF+eVMx5z+/FuIZwQZgr771Pdn8VPHRNkl2onc6Odup2I5unGMmyYuBtwNHV9WDE6qtLaOOeTfgOcCVSW6j15e6ap5fMB733/ZlVfXzqroVuJleMMxX4xzzicClAFV1FfA4eoOzLVRj/X/fGgsxCK4B9k2yT5Kd6V0MXjXQZhVwfLP8cuBL1VyFmadGHnPTTfJheiEw3/uNYcQxV9X9VbWkqpZV1TJ610WOrqq10yl3Tozzb/sz9G4MIMkSel1Ft0y0yrk1zjHfDhwOkOS36AXBxolWOVmrgNc0dw8dDNxfVXdvzxsuuK6hqno4ycnAFfTuOPhIVd2U5AxgbVWtAs6nd/q4nt6ZwDHTq3j7jXnM7wV2BT7ZXBe/vaqOnlrR22nMY15QxjzmK4CXJFkHPAK8rap+ML2qt8+Yx/wW4L8lOYVeF8kJ8/kXuySfoNe1t6S57vFO4LEAVXUuvesgRwHrgZ8Ar93uz5zHf16SpDmwELuGJElbwSCQpI4zCCSp4wwCSeo4g0CSOs4g0A5v1GiMTZu3N6Nt3pDkuiQHzXENq5M8sVl+U5LvJvl4kqNnG/m0af+N5vuyJK8a8/P+TZLTm+V3JbmzOa7rkpzZbL+yGZXz+mZIiWcO2X5Nkv373vcLSZ60bX8KWqi8fVQ7vCQvAH5Mb6Ct5wzZfwjw98BhVfVg8yDVzlXVyuBjSf4ncGTz5O7WvO4w4K1V9SdjtP0GvQfg7k3yLuDHVfV3A22ubN5vbZKTgD+pqqMHtr8WeFVV/VHzmuOBPavq3VtTuxY2zwi0wxtjNMbdgXs3DZtRVfduCoEktyX52yTfbL6e0WxfmuTTzW/M1yQ5tNm+a5J/SPKd5uziZX3vsyTJufSGRF6V5JQkJyT5YNPmN5L89+Y38euT/H6z/cdNnWcCz29+qz8lydcGflv/epLnJtkPeLCq7t2KP6avAs8Ysv0q/vWAZKvojc0jPcog0ELwOWCvJN9L8qEkgwPqPVBVBwIfBN7XbHs/cHZV/R7wMuC8Zvtf0Xtk/7er6rnAl/rfqKreQG9clxdV1dkDn/MB4CtV9Tv0xpO/aWD/acDXqmr/5rXn0QyK1/zw36WqbgAOBb418NpT+rqG/njIn8FLge8M2X4EvWEnNtV/H7BLkvk82q7m2IIbYkLdU1U/TvK7wPPpjbNzSZLTquqCpskn+r5v+uH9YmB5fjkNxeIkuzXbHx1yZNO4/mP6Q+A1zeseoTe8+Ww+CfxVkrcB/xHYVO/ubD5WztmDXUONjyf5KXAb/3r45Y8n+VV6wzIMzmB1D/BUYN4OPaG5ZRBo3kmyF/DPzeq5VXVu84P3SnqjjX6H3qCCFzRt+i+EbVp+DHBIVf104L3DhIYkr6qfJPk8vYlGXkFveHCAn9IbEXccr97CQHqvpjeb15nAOcC/69v3uOYzJMCuIc1DVXVH072yf1Wdm+SZSfqHWt4f+H7f+iv7vl/VLH8OOHlTg76++sHtW3OHzRfpTQNKkkVJFg/s/xG94bH7nUevS+maqtp0HeS7DO/v3ypV9XPgHcDB6Y3KuSnonkLvDEICDALNA81ojFcBz0yyIcmJA012BS5Mb8L2G+jNW/uuvv27JFkDvBk4pdn2JmCmuSC8DnhDs/1vgCcluTHJ9TRDOo/pzcCLmjOSa4FnD+y/AXi4uZB8CkBVXQs8APxDX7uvAgck2z99anPGcxbw1mbT7wJXNzPzSYC3j2qBS29SmpmtvANnYpI8lV6X1rP6Z9RK8n7gn6vqC3P8ee8HVlXVF+fyfTW/eUYgTUmS1wBrgLcPmVbxvwCPb+FjbzQENMgzAknqOM8IJKnjDAJJ6jiDQJI6ziCQpI4zCCSp4/4/i6u/EnpXvDUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = fpr\n",
    "y = tpr\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('1-Specificity(FPR)')\n",
    "plt.ylabel('Sensitivity(TPR)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {thrs_i: abs(fpr_i - tpr_i) for fpr_i, tpr_i, thrs_i in zip(fpr,tpr,thresholds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.9999999627788656: 0.0,\n",
       " 0.9999999627788656: 0.1111111111111111,\n",
       " 0.999974307572721: 0.2222222222222222,\n",
       " 0.9987101047553517: 0.22208159502492225,\n",
       " 0.9614878302795403: 0.33319270613603336,\n",
       " 0.6377949793622499: 0.33291145174143344,\n",
       " 0.5480714724249394: 0.44402256285254454,\n",
       " 0.3677436901126066: 0.4437413084579446,\n",
       " 0.3611646717614541: 0.5548524195690558,\n",
       " 0.1364751087197659: 0.5540086563852561,\n",
       " 0.1340164036437762: 0.6651197674963671,\n",
       " 0.08675003681059006: 0.6637134955233676,\n",
       " 0.07940403150521279: 0.7748246066344787,\n",
       " 0.03257228184080884: 0.7689182643478805,\n",
       " 0.03223047153492957: 0.8800293754589915,\n",
       " 0.013500226499343038: 0.8611853310207972,\n",
       " 0.013377118655341913: 0.9722964421319084,\n",
       " 0.0005966328645439909: 0.6451975812122064,\n",
       " 0.0005963165285946924: 0.6449163268176066,\n",
       " 5.447123249424493e-12: 0.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res \n",
    "# 1 이상의 확률값이 나오는 이유는 fpr = 0, tpr = 0으로 만들기 위함\n",
    "# abs(fpr - tpr) 값이 클 수록 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " res_sorted = sorted(res.items(), key=lambda t: t[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013377118655341913"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_cutoff = res_sorted[0][0] # 가장 abs(fpr - tpr) 값이 큰 thresholds를 리턴함\n",
    "optimal_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 cut_off로 예측하기\n",
    "y_pred_new = logreg.predict_proba(X_test)[:,1] >= optimal_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = list(map(int, y_pred_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6914,  197],\n",
       "       [   0,    9]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_new) # FN 문제를 해결함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred_new) # recall == 1 을 보여줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결론\n",
    "1. target 값의 분포가 균형이 깨져있는 경우, 모델의 성능 평가 지표로 Accuracy를 사용하는 것이 부적합하다.\n",
    "2. 사용한 데이터의 특징에 따라 어떤 성능 평가 지표를 사용할지에 대한 고민이 필요하다. 가령, 질병 예측 혹은 사기 탐지(이상치 탐지) 등의 경우에는 False Negative Case를 줄이고, True Positive Case를 높이는 것이 중요하다. (ex. 질병에 걸렸으나 질병에 걸리지 않았다고 하는 경우, 질병에 걸리지 않았으나 질병에 걸렸다고 하는 경우보다 risk가 크기 때문)\n",
    "3. ROC_AUC 커브의 사용법을 익히고 이에 따라 optimal cut-off를 결정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
